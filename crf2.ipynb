{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subrata.a.ghosh/Library/Python/3.6/lib/python/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/Users/subrata.a.ghosh/.matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/Users/subrata.a.ghosh/Library/Python/3.6/lib/python/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/Users/subrata.a.ghosh/.matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import xml.etree.ElementTree as ET #parse xml file\n",
    "import pandas as pd \n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml(root):\n",
    "    all_aspectTerms = [] #only aspect terms \n",
    "    all_text_with_boi = [] #review with aspect terms\n",
    "\n",
    "    for sentences in root.findall('sentence'):\n",
    "        text = sentences.find('text').text\n",
    "        aspectTerms = sentences.find('aspectTerms')\n",
    "        asp = []\n",
    "        try: #Because, review text might not any aspect term\n",
    "            for neighbor in aspectTerms.iter('aspectTerm'):\n",
    "                asp1 = neighbor.attrib.get('term',\"\")\n",
    "                #all_aspectTerms.append(asp1)\n",
    "                asp.append(asp1)\n",
    "            as_word_boi = BIO(text, asp)\n",
    "            all_aspectTerms.append(asp)\n",
    "            all_text_with_boi.append(as_word_boi)\n",
    "        except:\n",
    "            as_word_boi = BIO(text, asp)\n",
    "            all_text_with_boi.append(as_word_boi)\n",
    "            all_aspectTerms.append(asp)\n",
    "\n",
    "\n",
    "    return (all_text_with_boi,all_aspectTerms)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIO(test_text, aspect_terms):\n",
    "    all_word = {}\n",
    "    #split_test = word_tokenize(test_text)\n",
    "    split_test = test_text.split()\n",
    "    pos_tag_ = pos_tag(split_test)\n",
    "    for word in split_test:\n",
    "        all_word[word] = \"O\"\n",
    "    if len(aspect_terms)>0:\n",
    "        for aspect in aspect_terms:\n",
    "            fl=0\n",
    "            for asp in aspect.split():\n",
    "                for word in split_test:\n",
    "                    if asp.lower() in word.lower():\n",
    "                        #print (asp,word)\n",
    "                        asp = word\n",
    "                        break\n",
    "                if fl==0:            \n",
    "                    all_word[asp] = \"B\" \n",
    "                    fl = 1\n",
    "                else:\n",
    "                    all_word[asp] = \"I\"\n",
    "    li = []\n",
    "    for word, cls in all_word.items():\n",
    "        for word1,tag in pos_tag_:\n",
    "            if word == word1:\n",
    "                li.append((word,cls,tag))\n",
    "                break\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'O', 'PRP'), ('charge', 'O', 'VBP'), ('it', 'O', 'PRP'), ('at', 'O', 'IN'), ('night', 'O', 'NN'), ('and', 'O', 'CC'), ('skip', 'O', 'NN'), ('taking', 'O', 'VBG'), ('the', 'O', 'DT'), ('cord', 'B', 'NN'), ('with', 'O', 'IN'), ('me', 'O', 'PRP'), ('because', 'O', 'IN'), ('of', 'O', 'IN'), ('good', 'O', 'JJ'), ('battery', 'B', 'NN'), ('life.', 'I', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "train_tree = ET.parse('Laptops_Train_v2.xml')\n",
    "train_root = train_tree.getroot()\n",
    "\n",
    "training,all_aspectTerms = parse_xml(train_root)\n",
    "\n",
    "print (training[0]) # format = (review text, [aspect trams,....])Restaurants_Train.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Boot', 'B', 'NNP'), ('time', 'I', 'NN'), ('is', 'O', 'VBZ'), ('super', 'O', 'JJ'), ('fast,', 'O', 'NN'), ('around', 'O', 'RB'), ('anywhere', 'O', 'RB'), ('from', 'O', 'IN'), ('35', 'O', 'CD'), ('seconds', 'O', 'NNS'), ('to', 'O', 'TO'), ('1', 'O', 'CD'), ('minute.', 'O', 'NNS')]\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "test_tree = ET.parse('Laptops_Test_Gold.xml')\n",
    "test_root = test_tree.getroot()\n",
    "\n",
    "test,all_aspectTerms = parse_xml(test_root)\n",
    "\n",
    "print (test[0]) # format = (review text, [aspect trams,....])\n",
    "print (len(all_aspectTerms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(tagged, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    sentence = [tag for tag,a,b in tagged]\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'tag': tagged[index][2],\n",
    "        'prev_prev_tag': '' if index <= 2 else tagged[index - 2][2],\n",
    "        'prev_tag': '' if index == 0 else tagged[index - 1][2],\n",
    "        'next_tag': '' if index == len(sentence) - 1 else tagged[index + 1][2],\n",
    "        'next_next_tag': '' if index >= len(sentence) - 2 else tagged[index + 2][2],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3045\n",
      "800\n",
      "[{'word': 'I', 'tag': 'PRP', 'prev_prev_tag': '', 'prev_tag': '', 'next_tag': 'VBP', 'next_next_tag': 'PRP', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'I', 'prefix-2': 'I', 'prefix-3': 'I', 'suffix-1': 'I', 'suffix-2': 'I', 'suffix-3': 'I', 'prev_word': '', 'next_word': 'charge', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'charge', 'tag': 'VBP', 'prev_prev_tag': '', 'prev_tag': 'PRP', 'next_tag': 'PRP', 'next_next_tag': 'IN', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'c', 'prefix-2': 'ch', 'prefix-3': 'cha', 'suffix-1': 'e', 'suffix-2': 'ge', 'suffix-3': 'rge', 'prev_word': 'I', 'next_word': 'it', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'it', 'tag': 'PRP', 'prev_prev_tag': '', 'prev_tag': 'VBP', 'next_tag': 'IN', 'next_next_tag': 'NN', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'i', 'prefix-2': 'it', 'prefix-3': 'it', 'suffix-1': 't', 'suffix-2': 'it', 'suffix-3': 'it', 'prev_word': 'charge', 'next_word': 'at', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'at', 'tag': 'IN', 'prev_prev_tag': 'VBP', 'prev_tag': 'PRP', 'next_tag': 'NN', 'next_next_tag': 'CC', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'a', 'prefix-2': 'at', 'prefix-3': 'at', 'suffix-1': 't', 'suffix-2': 'at', 'suffix-3': 'at', 'prev_word': 'it', 'next_word': 'night', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'night', 'tag': 'NN', 'prev_prev_tag': 'PRP', 'prev_tag': 'IN', 'next_tag': 'CC', 'next_next_tag': 'NN', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'n', 'prefix-2': 'ni', 'prefix-3': 'nig', 'suffix-1': 't', 'suffix-2': 'ht', 'suffix-3': 'ght', 'prev_word': 'at', 'next_word': 'and', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'and', 'tag': 'CC', 'prev_prev_tag': 'IN', 'prev_tag': 'NN', 'next_tag': 'NN', 'next_next_tag': 'VBG', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'a', 'prefix-2': 'an', 'prefix-3': 'and', 'suffix-1': 'd', 'suffix-2': 'nd', 'suffix-3': 'and', 'prev_word': 'night', 'next_word': 'skip', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'skip', 'tag': 'NN', 'prev_prev_tag': 'NN', 'prev_tag': 'CC', 'next_tag': 'VBG', 'next_next_tag': 'DT', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 's', 'prefix-2': 'sk', 'prefix-3': 'ski', 'suffix-1': 'p', 'suffix-2': 'ip', 'suffix-3': 'kip', 'prev_word': 'and', 'next_word': 'taking', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'taking', 'tag': 'VBG', 'prev_prev_tag': 'CC', 'prev_tag': 'NN', 'next_tag': 'DT', 'next_next_tag': 'NN', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 't', 'prefix-2': 'ta', 'prefix-3': 'tak', 'suffix-1': 'g', 'suffix-2': 'ng', 'suffix-3': 'ing', 'prev_word': 'skip', 'next_word': 'the', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'the', 'tag': 'DT', 'prev_prev_tag': 'NN', 'prev_tag': 'VBG', 'next_tag': 'NN', 'next_next_tag': 'IN', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 't', 'prefix-2': 'th', 'prefix-3': 'the', 'suffix-1': 'e', 'suffix-2': 'he', 'suffix-3': 'the', 'prev_word': 'taking', 'next_word': 'cord', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'cord', 'tag': 'NN', 'prev_prev_tag': 'VBG', 'prev_tag': 'DT', 'next_tag': 'IN', 'next_next_tag': 'PRP', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'c', 'prefix-2': 'co', 'prefix-3': 'cor', 'suffix-1': 'd', 'suffix-2': 'rd', 'suffix-3': 'ord', 'prev_word': 'the', 'next_word': 'with', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'with', 'tag': 'IN', 'prev_prev_tag': 'DT', 'prev_tag': 'NN', 'next_tag': 'PRP', 'next_next_tag': 'IN', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'w', 'prefix-2': 'wi', 'prefix-3': 'wit', 'suffix-1': 'h', 'suffix-2': 'th', 'suffix-3': 'ith', 'prev_word': 'cord', 'next_word': 'me', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'me', 'tag': 'PRP', 'prev_prev_tag': 'NN', 'prev_tag': 'IN', 'next_tag': 'IN', 'next_next_tag': 'IN', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'm', 'prefix-2': 'me', 'prefix-3': 'me', 'suffix-1': 'e', 'suffix-2': 'me', 'suffix-3': 'me', 'prev_word': 'with', 'next_word': 'because', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'because', 'tag': 'IN', 'prev_prev_tag': 'IN', 'prev_tag': 'PRP', 'next_tag': 'IN', 'next_next_tag': 'JJ', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'b', 'prefix-2': 'be', 'prefix-3': 'bec', 'suffix-1': 'e', 'suffix-2': 'se', 'suffix-3': 'use', 'prev_word': 'me', 'next_word': 'of', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'of', 'tag': 'IN', 'prev_prev_tag': 'PRP', 'prev_tag': 'IN', 'next_tag': 'JJ', 'next_next_tag': 'NN', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'o', 'prefix-2': 'of', 'prefix-3': 'of', 'suffix-1': 'f', 'suffix-2': 'of', 'suffix-3': 'of', 'prev_word': 'because', 'next_word': 'good', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'good', 'tag': 'JJ', 'prev_prev_tag': 'IN', 'prev_tag': 'IN', 'next_tag': 'NN', 'next_next_tag': 'NN', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'g', 'prefix-2': 'go', 'prefix-3': 'goo', 'suffix-1': 'd', 'suffix-2': 'od', 'suffix-3': 'ood', 'prev_word': 'of', 'next_word': 'battery', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'battery', 'tag': 'NN', 'prev_prev_tag': 'IN', 'prev_tag': 'JJ', 'next_tag': 'NN', 'next_next_tag': '', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'b', 'prefix-2': 'ba', 'prefix-3': 'bat', 'suffix-1': 'y', 'suffix-2': 'ry', 'suffix-3': 'ery', 'prev_word': 'good', 'next_word': 'life.', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'life.', 'tag': 'NN', 'prev_prev_tag': 'JJ', 'prev_tag': 'NN', 'next_tag': '', 'next_next_tag': '', 'is_first': False, 'is_last': True, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'l', 'prefix-2': 'li', 'prefix-3': 'lif', 'suffix-1': '.', 'suffix-2': 'e.', 'suffix-3': 'fe.', 'prev_word': 'battery', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}]\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'I']\n"
     ]
    }
   ],
   "source": [
    "#from nltk.tag.util import untag\n",
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    " \n",
    "    for tagged in tagged_sentences:\n",
    "        X.append([features(tagged, index) for index in range(len(tagged))])\n",
    "        y.append([a for _,a,tag in tagged])\n",
    " \n",
    "    return X, y\n",
    " \n",
    "X_train, y_train = transform_to_dataset(training)\n",
    "X_test, y_test = transform_to_dataset(test)\n",
    " \n",
    "print(len(X_train))     \n",
    "print(len(X_test))         \n",
    "print(X_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='l2sgd', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None, c1=None, c2=0.01,\n",
       "  calibration_candidates=None, calibration_eta=0.01,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=3000,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    " \n",
    "model = CRF(\n",
    "    algorithm='l2sgd',\n",
    "    #c1=0.1,\n",
    "    c2=0.01,\n",
    "    #period=20,\n",
    "    calibration_eta=0.01,\n",
    "    max_iterations=3000,\n",
    "    all_possible_transitions=True,\n",
    "    #all_possible_states=True \n",
    ")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='l2sgd', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None, c1=None, c2=0.01,\n",
       "  calibration_candidates=None, calibration_eta=0.01,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=3000,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9452082905807511\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.78      0.61      0.68       639\n",
      "           I       0.85      0.49      0.62       393\n",
      "           O       0.96      0.99      0.97      8714\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      9746\n",
      "   macro avg       0.86      0.70      0.76      9746\n",
      "weighted avg       0.94      0.95      0.94      9746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn_crfsuite import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(metrics.flat_accuracy_score(y_test, y_pred))\n",
    "print(metrics.flat_classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 14.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 40s, sys: 8.41 s, total: 13min 48s\n",
      "Wall time: 14min 59s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "# define fixed parameters and parameters to search\n",
    "crf = CRF(\n",
    "    algorithm='l2sgd',\n",
    "    max_iterations=3000,\n",
    "    all_possible_transitions=True,\n",
    "    all_possible_states=True\n",
    ")\n",
    "params_space = {\n",
    "    'calibration_eta': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted')\n",
    "\n",
    "# search\n",
    "model = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'c2': 0.11408738555005796, 'calibration_eta': 1.0620521205678686}\n",
      "best CV score: 0.9506949414571738\n",
      "model size: 3.17M\n"
     ]
    }
   ],
   "source": [
    "# crf = rs.best_estimator_\n",
    "print('best params:', model.best_params_)\n",
    "print('best CV score:', model.best_score_)\n",
    "print('model size: {:0.2f}M'.format(model.best_estimator_.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.best_estimator_\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_pred_asp = []\n",
    "for i in range(len(y_pred)):\n",
    "    asp_list = []\n",
    "    asp = \"\"\n",
    "    fl = 0\n",
    "    if(359):\n",
    "        for j in range(len(y_pred[i])):\n",
    "            if y_pred[i][j] == \"B\" and fl ==0:\n",
    "                asp += test[i][j][0]\n",
    "                fl=1\n",
    "                #print(asp)\n",
    "            elif(y_pred[i][j] == \"B\" and fl ==1):\n",
    "                asp_list.append(asp)\n",
    "                asp = \"\"\n",
    "                asp += test[i][j][0]\n",
    "                fl=1\n",
    "            elif(y_pred[i][j] == \"I\" and fl==1):\n",
    "                asp += \" \"+ test[i][j][0]\n",
    "                #print(asp)\n",
    "            elif(y_pred[i][j] == \"I\" and fl==0):\n",
    "                asp += test[i][j][0]\n",
    "                fl=1\n",
    "            else:\n",
    "                if fl==1:\n",
    "                    asp_list.append(asp)\n",
    "                fl=0\n",
    "                asp = \"\"\n",
    "        if fl==1:\n",
    "            asp_list.append(asp)\n",
    "        all_pred_asp.append(asp_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354\n",
      "0.708\n",
      "0.5412844036697247\n",
      "0.6135181975736569\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "p = 0\n",
    "r = 0\n",
    "f = open(\"crf_asp.txt\",'w')\n",
    "for i in range(len(all_pred_asp)):\n",
    "    pre = [x.lower().replace(\".\", \"\").replace(\",\", \"\") for x in all_pred_asp[i]]\n",
    "    act = [x.lower() for x in all_aspectTerms[i]]\n",
    "    #for asp in pre:\n",
    "        #if asp in act:\n",
    "            #c+=1\n",
    "    c += len([a for a in pre if a in act])\n",
    "    #print (pre)\n",
    "    #print (act)\n",
    "    p += len(all_pred_asp[i])\n",
    "    r += len(all_aspectTerms[i])\n",
    "    \n",
    "    ss = \"\"\n",
    "    for token in pre:\n",
    "        ss+=token+'/'\n",
    "    f.write(ss)\n",
    "    f.write('\\n')\n",
    "    \n",
    "    #break\n",
    "f.close()\n",
    "print (c)\n",
    "print (float(c/p))\n",
    "print (float(c/r))\n",
    "print ((2*float(c/p)*float(c/r))/(float(c/p)+float(c/r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
